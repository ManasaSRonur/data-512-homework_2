{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA 512 Homework 2: Considering Bias in Data\n",
    "\n",
    "### Project Overview\n",
    "The goal of this assignment is to explore the concept of bias in data using Wikipedia articles. We will consider articles on political figures from different countries. The idea is to combine a dataset of Wikipedia articles with a dataset of country populations, and use a machine learning service called ORES to estimate the quality of each article. And then perform an analysis of how the coverage of politicians on Wikipedia and the quality of articles about politicians varies among countries. \n",
    "\n",
    "### License\n",
    "\n",
    "#### Code Attribution\n",
    "\n",
    "Snippets of the code were taken from a code example developed by **Dr. David W. McDonald** for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided under the **Creative Commons CC-BY license**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below section, importing the libraries that are necessary to work with API calls, parse data and save data into required file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# These are standard python modules\n",
    "import json, time, urllib.parse\n",
    "#\n",
    "# The 'requests' module is not a standard Python module. You will need to install this with pip/pip3 if you do not already have it\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data acquisition code relies on some constants that help make the code a bit more readable and flexible. These include request headers, API parameters, name and format of the files to be generated later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# The basic English Wikipedia API endpoint\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "API_HEADER_AGENT = 'User-Agent'\n",
    "\n",
    "# We'll assume that there needs to be some throttling for these requests - we should always be nice to a free data resource\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "# This should include an email - your UW email would be good to put in there\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<manasars@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2024'\n",
    "}\n",
    "\n",
    "# This is a string of additional page properties that can be returned see the Info documentation for\n",
    "# what can be included. If you don't want any this can simply be the empty string\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\n",
    "#PAGEINFO_EXTENDED_PROPERTIES = \"\"\n",
    "\n",
    "# This template lists the basic parameters for making this\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",           # to simplify this should be a single page title at a time\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}\n",
    "\n",
    "# Input file path containing article titles.\n",
    "ARTICLE_LIST_FILE = \"inputfiles/politicians_by_country_AUG.2024.csv\"\n",
    "POPULATION_FILE = \"inputfiles/population_by_country_AUG.2024.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API request will be made using one procedure. The idea is to make this reusable. The procedure is parameterized, but relies on the constants above for the important parameters. The underlying assumption is that this will be used to request data for a set of article pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_pageinfo_per_article(article_title = None, \n",
    "                                 endpoint_url = API_ENWIKIPEDIA_ENDPOINT, \n",
    "                                 request_template = PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                 headers = REQUEST_HEADERS):\n",
    "    \n",
    "    # article title can be as a parameter to the call or in the request_template\n",
    "    if article_title:\n",
    "        request_template['titles'] = article_title\n",
    "\n",
    "    if not request_template['titles']:\n",
    "        raise Exception(\"Must supply an article title to make a pageinfo request.\")\n",
    "\n",
    "    if API_HEADER_AGENT not in headers:\n",
    "        raise Exception(f\"The header data should include a '{API_HEADER_AGENT}' field that contains your UW email address.\")\n",
    "\n",
    "    if 'uwnetid@uw' in headers[API_HEADER_AGENT]:\n",
    "        raise Exception(f\"Use your UW email address in the '{API_HEADER_AGENT}' field.\")\n",
    "\n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or any other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csv library will be used to read article list from csv and store it in a variable for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This custom module will read the csv file from the path passed as argument, and store the article list in the temporary list variable to be used during API calls.The csv reader object treats each row as a dictionary, so we iterate over each row and access the value for key \"name\" which is the article name and append to article list variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read rare disease article titles from the CSV file\n",
    "def read_article_file(file_path):\n",
    "    \"\"\"\n",
    "    Read the article titles from a CSV file.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): Path to the CSV file containing article titles.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of article titles extracted from the 'disease' column of the CSV file.\n",
    "\n",
    "    \"\"\"\n",
    "    article_titles = []\n",
    "    try:\n",
    "        with open(file_path, mode='r', newline='', encoding='utf-8') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                if 'name' in row:\n",
    "                    article_titles.append(row['name']) \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: CSV file '{file_path}' not found. Please check the file path and try again.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error while reading '{file_path}': {e}\")\n",
    "        raise\n",
    "    return article_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below section, we will create a module to fetch page info data for articles. For each article in the list generated above, we retrieve page info data from the Wikimedia API and saves it to internediary file for later use. while processing we also make a list of articles failed to fetch the page info data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_and_save_data(articles):\n",
    "    \"\"\"\n",
    "    Process the Wikipedia API data for each article and save it as a CSV.\n",
    "\n",
    "    Args:\n",
    "    - article_titles (list): List of article titles to query from the Wikipedia API.\n",
    "    - output_file (str): Path to save the CSV output file.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    output_file=\"intermediary_files/articles_page_info.csv\"\n",
    "\n",
    "    # List to store each article's processed data\n",
    "    article_data = []\n",
    "    failed_to_process = []\n",
    "\n",
    "    for article in articles:\n",
    "        print(f\"Processing article: {article}\")\n",
    "\n",
    "        # Request data for the article\n",
    "        response = request_pageinfo_per_article(article)\n",
    "\n",
    "        if response is not None:\n",
    "        # Append the response to the list as a dictionary\n",
    "            article_data.append(response)\n",
    "        else:\n",
    "            print(f\"Failed to process{article}\")\n",
    "            failed_to_process.append(article)\n",
    "    \n",
    "    article_info_df = pd.DataFrame(article_data)\n",
    "\n",
    "    result_df = pd.DataFrame()\n",
    "\n",
    "    for index, row in article_info_df.iterrows():\n",
    "        page_info = row['query']\n",
    "        page_id = list(page_info['pages'].keys())[0]\n",
    "        page_data = page_info['pages'][page_id]\n",
    "\n",
    "        # Convert the nested page_data dictionary into a single row\n",
    "        page_data_df = pd.DataFrame.from_dict(page_data, orient='index').T\n",
    "        result_df = pd.concat([result_df, page_data_df])\n",
    "\n",
    "    # Write the data to a CSV file\n",
    "    try:\n",
    "        result_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        print(f\"Data successfully saved to {output_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data to CSV: {e}\")\n",
    "        print(failed_to_process)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use our custom modules to read the csv and store the article list in temporary variable and use it to read and process the data using the above defined custom module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the article titles from the CSV\n",
    "article_titles = read_article_file(ARTICLE_LIST_FILE)\n",
    "\n",
    "# Process and save data for all articles\n",
    "#process_and_save_data(article_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define the parameters and constants needed to fetch data from ORES APIs. Wikimedia is implementing a new Machine Learning (ML) service infrastructure that they call LiftWing. Given that ORES already has several ML models that have been well used, ORES is the first set of APIs that are being moved to LiftWing.\n",
    "\n",
    "Access to the ORES API will require that you request an API access key.\n",
    "\n",
    "To Get your access token: You will need a Wikimedia user account to get access to Lift Wing (the ML API service). You can either [create an account or login](https://api.wikimedia.org/w/index.php?title=Special:UserLogin&centralAuthAutologinTried=1&centralAuthError=Not+centrally+logged+in). If you have a Wikipedia user account - you might already have an Wikimedia account. If you are not sure try your Wikipedia username and password to check it. If you do not have a Wikimedia account you will need to create an account that you can use to get an access token.\n",
    "\n",
    "Here is the [guide](https://api.wikimedia.org/wiki/Authentication) provides detailed steps on generating access token.\n",
    "\n",
    "Note, when you create a Personal API token you are granted the three items - a Client ID, a Client secret, and a Access token - you shold save all three of these. When you dismiss the box they are gone. If you lose any one of the tokens you can destroy or deactivate the Personal API token from the dashboard and then create a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "#    The current LiftWing ORES API endpoint and prediction model\n",
    "#\n",
    "API_ORES_LIFTWING_ENDPOINT = \"https://api.wikimedia.org/service/lw/inference/v1/models/{model_name}:predict\"\n",
    "API_ORES_EN_QUALITY_MODEL = \"enwiki-articlequality\"\n",
    "\n",
    "#\n",
    "#    The throttling rate is a function of the Access token that you are granted when you request the token. The constants\n",
    "#    come from dissecting the token and getting the rate limits from the granted token. An example of that is below.\n",
    "#\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = ((60.0*60.0)/5000.0)-API_LATENCY_ASSUMED  # The key authorizes 5000 requests per hour\n",
    "\n",
    "#    When making automated requests we should include something that is unique to the person making the request\n",
    "#    This should include an email - your UW email would be good to put in there\n",
    "#    \n",
    "#    Because all LiftWing API requests require some form of authentication, you need to provide your access token\n",
    "#    as part of the header too\n",
    "#\n",
    "REQUEST_HEADER_TEMPLATE = {\n",
    "    'User-Agent': \"<manasars@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2024\",\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': \"Bearer {access_token}\"\n",
    "}\n",
    "#\n",
    "#    This is a template for the parameters that we need to supply in the headers of an API request\n",
    "#\n",
    "REQUEST_HEADER_PARAMS_TEMPLATE = {\n",
    "    'email_address' : \"\",         # your email address should go here\n",
    "    'access_token'  :  \"\"     # the access token you create will need to go here\n",
    "}\n",
    "\n",
    "#\n",
    "#    This is a template of the data required as a payload when making a scoring request of the ORES model\n",
    "#\n",
    "ORES_REQUEST_DATA_TEMPLATE = {\n",
    "    \"lang\":        \"en\",     # required that its english - we're scoring English Wikipedia revisions\n",
    "    \"rev_id\":      \"\",       # this request requires a revision id\n",
    "    \"features\":    True\n",
    "}\n",
    "\n",
    "#\n",
    "#    These are used later - defined here so they, at least, have empty values\n",
    "#\n",
    "EMAIL_ADDRESS = \"manasars@uw.edu\"\n",
    "USERNAME = \"\"\n",
    "ACCESS_TOKEN = \"\"\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define function to make the ORES API request.\n",
    "\n",
    "The API request will be made using a function to encapsulate call and make access reusable in other notebooks. The procedure is parameterized, relying on the constants above for some important default parameters. The primary assumption is that this function will be used to request data for a set of article revisions. The main parameter is 'article_revid'. One should be able to simply pass in a new article revision id on each call and get back a python dictionary as the result. A valid result will be a dictionary that contains the probabilities that the specific revision is one of six different article quality levels. Generally, quality level with the highest probability score is considered the quality level for the article. This can be tricky when you have two (or more) highly probable quality levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_ores_score_per_article(article_revid = None, email_address=None, access_token=None,\n",
    "                                   endpoint_url = API_ORES_LIFTWING_ENDPOINT, \n",
    "                                   model_name = API_ORES_EN_QUALITY_MODEL, \n",
    "                                   request_data = ORES_REQUEST_DATA_TEMPLATE, \n",
    "                                   header_format = REQUEST_HEADER_TEMPLATE, \n",
    "                                   header_params = REQUEST_HEADER_PARAMS_TEMPLATE):\n",
    "    \n",
    "    #    Make sure we have an article revision id, email and token\n",
    "    #    This approach prioritizes the parameters passed in when making the call\n",
    "    if article_revid:\n",
    "        request_data['rev_id'] = article_revid\n",
    "    if email_address:\n",
    "        header_params['email_address'] = email_address\n",
    "    if access_token:\n",
    "        header_params['access_token'] = access_token\n",
    "    \n",
    "    #   Making a request requires a revision id - an email address - and the access token\n",
    "    if not request_data['rev_id']:\n",
    "        raise Exception(\"Must provide an article revision id (rev_id) to score articles\")\n",
    "    if not header_params['email_address']:\n",
    "        raise Exception(\"Must provide an 'email_address' value\")\n",
    "    if not header_params['access_token']:\n",
    "        raise Exception(\"Must provide an 'access_token' value\")\n",
    "    \n",
    "    # Create the request URL with the specified model parameter - default is a article quality score request\n",
    "    request_url = endpoint_url.format(model_name=model_name)\n",
    "    \n",
    "    # Create a compliant request header from the template and the supplied parameters\n",
    "    headers = dict()\n",
    "    for key in header_format.keys():\n",
    "        headers[str(key)] = header_format[key].format(**header_params)\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free data\n",
    "        # source like ORES - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        #response = requests.get(request_url, headers=headers)\n",
    "        response = requests.post(request_url, headers=headers, data=json.dumps(request_data))\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below custom module will fetch and process the ORES scores data for each article with valid last revision id using the ORES API function defined above. \n",
    "\n",
    "For each article, the script reads the corresponding last revision id and uses it to make a request to the ORES API to obtain the quality score for that article. If the quality score is successfully retrieved, it is saved into the intermediary csv file for later use.\n",
    "If for any reason, ORES requests fails for an article or if the last revision id is not available for an article, then details of such articles are recorded. This information is used to calculate and display error rate of ORES scores fetch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_and_save_scores_data(file_path, email_address, access_token):\n",
    "    \"\"\"\n",
    "    Get ORES scores for a list of article revision IDs and save them to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file containing article page information.\n",
    "        email_address (str): Your email address for the API request.\n",
    "        access_token (str): Your access token for the API request.\n",
    "    \"\"\"\n",
    "    \n",
    "    failed_article_titles = []\n",
    "    output_csv = 'intermediary_files/articles_ores_scores.csv'\n",
    "    all_article_scores = []\n",
    "    total_articles = 0\n",
    "\n",
    "    try:\n",
    "        with open(file_path, mode='r', newline='', encoding='utf-8') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            \n",
    "            for row in reader:\n",
    "                total_articles += 1 \n",
    "                if 'lastrevid' in row and row['lastrevid'].strip():\n",
    "                    rev_id = int(row['lastrevid'])\n",
    "                    print(f\"Requesting ORES score for revision ID: {rev_id}\")\n",
    "                    response = request_ores_score_per_article(article_revid=rev_id,\n",
    "                                                    email_address=email_address,\n",
    "                                                    access_token=access_token)\n",
    "                    if response is not None:\n",
    "                        # Initialize score data with revision ID and prediction\n",
    "                        score_data = {\n",
    "                            'revision_id': rev_id,\n",
    "                            'quality_prediction': response.get('enwiki', {}).get('scores', {}).get(str(rev_id), {}).get('articlequality', {}).get('score', {}).get('prediction')\n",
    "                        }\n",
    "                        \n",
    "                        # Extract probabilities and flatten them into score_data\n",
    "                        probabilities = response.get('enwiki', {}).get('scores', {}).get(str(rev_id), {}).get('articlequality', {}).get('score', {}).get('probability', {})\n",
    "                        score_data.update({f'Probability {key}': value for key, value in probabilities.items()})\n",
    "\n",
    "                        all_article_scores.append(score_data)\n",
    "\n",
    "                    else:\n",
    "                        print(f\"Failed to process {row['title']}\")\n",
    "                        failed_article_titles.append(row['title'])\n",
    "                \n",
    "                else:\n",
    "                    print(f\"Revision_id not available {row['title']}\")\n",
    "                    failed_article_titles.append(row['title'])\n",
    "            \n",
    "\n",
    "        # Convert the list of scores to a DataFrame\n",
    "        all_article_scores_df = pd.DataFrame(all_article_scores)\n",
    "\n",
    "        # Save to CSV\n",
    "        all_article_scores_df.to_csv(output_csv, index=False)\n",
    "        print(f\"Scores saved to {output_csv}\")\n",
    "\n",
    "        # Calculate and print error rate\n",
    "        error_count = len(failed_article_titles)  # Count of failed articles\n",
    "        error_rate = error_count / total_articles if total_articles > 0 else 0  # Avoid division by zero\n",
    "        print(f\"Error rate: {error_rate:.2%} ({error_count} failed out of {total_articles})\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: CSV file '{file_path}' not found. Please check the file path and try again.\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the custom module to fetch the scores info and save it to the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_page_info_file = \"intermediary_files/articles_page_info.csv\"\n",
    "#process_and_save_scores_data(path_to_page_info_file, EMAIL_ADDRESS, ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For analysis, the Wikipedia data and population will be merged together. For this, we will need to ensure both the datasets have column named \"Country\". First step is to fetch \"Country\"column from \"Geography\" column. since region is distinguished from country through All caps text, we will use the same logic to separate out region and country into two columns. Extract out all caps values and store them as separate column \"Region\", this leaves us with \"Geography\" column having only country values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "population_df = pd.read_csv(POPULATION_FILE)\n",
    "\n",
    "# Create a new 'Region' column with default values as empty strings\n",
    "population_df['Region'] = ''\n",
    "\n",
    "# Iterate through the rows to assign region values based on the 'Geography' column\n",
    "current_region = ''\n",
    "for index, row in population_df.iterrows():\n",
    "    # Check if the 'Geography' value is in ALL CAPS (indicating a region)\n",
    "    if row['Geography'].isupper():\n",
    "        current_region = row['Geography']\n",
    "    else:\n",
    "        population_df.at[index, 'Region'] = current_region\n",
    "\n",
    "# Remove rows where 'Geography' is in all caps, since these are now stored as 'Region' labels\n",
    "population_df = population_df[~population_df['Geography'].str.isupper()]\n",
    "\n",
    "# Rename the 'Geography' column to 'Country'\n",
    "population_df.rename(columns={'Geography': 'country'}, inplace=True)\n",
    "\n",
    "# Reset index if desired\n",
    "population_df.reset_index(drop=True, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step in the merge, we now have to have \"Country\" column in the [articles_ores_scores.csv]() file. Since this file is generated based on the article list, we will propogate the \"country\" column from [politicians_by_country.AUG.2024.csv](), all the way up to this ores scores file. This done by merging the files generated at every step of data processing. First merge will be between input file and page info csv file. Second merge will be between first merge and the ores scores. This will ensure we have required \"country\" column. The final merge is for our analysis, between wikipedis ORES scores and population data.\n",
    "\n",
    "As we do final merge, there may be countries with no matching entry in the other dataset, we will record such country names and store it ina  txt file, [wp_countries-no_match.txt]().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmatched countries saved to wp_countries-no_match.txt'.\n",
      "\n",
      "Merged data saved into wp_politicians_by_country.csv.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "politicians_df = pd.read_csv(ARTICLE_LIST_FILE)\n",
    "page_info_df = pd.read_csv(path_to_page_info_file)\n",
    "path_to_scores_file = \"intermediary_files/articles_ores_scores.csv\"\n",
    "page_score_df = pd.read_csv(path_to_scores_file)\n",
    "\n",
    "# First merge\n",
    "merged_df_1 = pd.merge(politicians_df, page_info_df, left_on='name', right_on='title', how='inner', indicator='merge_status_1')\n",
    "\n",
    "# Second merge with a new indicator column name\n",
    "merged_df_2 = pd.merge(merged_df_1, page_score_df, left_on='lastrevid', right_on='revision_id', how='inner', indicator='merge_status_2')\n",
    "\n",
    "merged_df = pd.merge(merged_df_2, population_df, on='country', how='outer', indicator='merge_status_3')\n",
    "\n",
    "no_match_wp = merged_df[merged_df['merge_status_3'] == 'left_only']['country'].unique()\n",
    "no_match_pop = merged_df[merged_df['merge_status_3'] == 'right_only']['country'].unique()\n",
    "\n",
    "    # Combine no-match countries from both sides\n",
    "no_match_countries = set(no_match_wp) | set(no_match_pop)\n",
    "\n",
    "    # Write unmatched countries to a text file\n",
    "with open(\"generated_files/wp_countries-no_match.txt\", 'w') as f:\n",
    "    for country in no_match_countries:\n",
    "        f.write(f\"{country}\\n\")\n",
    "\n",
    "print(f\"Unmatched countries saved to wp_countries-no_match.txt'.\")\n",
    "\n",
    "final_df = pd.merge(merged_df_2, population_df, on='country', how='inner', indicator='merge_status_3')\n",
    "final_df = final_df[['country', 'Region', 'Population', 'title', 'revision_id', 'quality_prediction']]\n",
    "final_df.to_csv(\"generated_files/wp_politicians_by_country.csv\", index=False)\n",
    "\n",
    "print(\"\\nMerged data saved into wp_politicians_by_country.csv.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For analysis, we are interested in total articles per capita and high quality articles per capita. To have the meaningful calculations at country level, we will exclude any countries where the countries have 0 millions as populatin data.\n",
    "\n",
    "The table genererated provides the \"Top 10 countries by coverage\": The 10 countries with the highest total articles per capita (in descending order) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Countries by Coverage:\n",
      "            Country  Total Articles per Capita\n",
      "0          Albania                   0.000027\n",
      "1          Bahrain                   0.000027\n",
      "2        Lithuania                   0.000023\n",
      "3          Moldova                   0.000021\n",
      "4           Kosovo                   0.000021\n",
      "5         Slovenia                   0.000020\n",
      "6          Croatia                   0.000019\n",
      "7  North Macedonia                   0.000017\n",
      "8         Djibouti                   0.000016\n",
      "9           Serbia                   0.000015\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Consider \"FA\" and \"GA\" as high-quality articles\n",
    "high_quality_classes = [\"FA\", \"GA\"]\n",
    "\n",
    "Valid_country_df = final_df[final_df['Population'].notna() & (final_df['Population'] >= 1)]\n",
    "\n",
    "# Calculate article counts per country and region\n",
    "country_total_articles = Valid_country_df.groupby('country').size()\n",
    "country_high_quality_articles = Valid_country_df[Valid_country_df['quality_prediction'].isin(high_quality_classes)].groupby('country').size()\n",
    "\n",
    "# Calculate total articles and high-quality articles per capita (per million people) for each country\n",
    "country_population = Valid_country_df.groupby('country')['Population'].mean()\n",
    "country_total_articles_per_capita = ( country_total_articles / (country_population * 1e6)).sort_values(ascending=False)\n",
    "\n",
    "#final_df['Population'] = final_df['Population'].replace(0, np.nan)\n",
    "\n",
    "# Generate the tables for top and bottom 10 countries by coverage and high quality\n",
    "top_10_countries_by_coverage = country_total_articles_per_capita.head(10)\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "top_10_countries_by_coverage_df = top_10_countries_by_coverage.reset_index()\n",
    "top_10_countries_by_coverage_df.columns = ['Country', 'Total Articles per Capita']\n",
    "\n",
    "# Print top 10 countries by coverage and their total articles per capita\n",
    "print(\"Top 10 Countries by Coverage:\\n\", top_10_countries_by_coverage_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table genererated provides the \"Bottom 10 countries by coverage\": The 10 countries with the lowest total articles per capita (in ascending order) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottom 10 Countries by Coverage: \n",
      "          Country  Total Articles per Capita\n",
      "0          China               1.133707e-08\n",
      "1          Ghana               8.797654e-08\n",
      "2          India               1.056979e-07\n",
      "3   Saudi Arabia               1.355014e-07\n",
      "4         Zambia               1.485149e-07\n",
      "5         Norway               1.818182e-07\n",
      "6         Israel               2.040816e-07\n",
      "7          Egypt               3.041825e-07\n",
      "8  Cote d'Ivoire               3.236246e-07\n",
      "9     Mozambique               3.539823e-07 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "bottom_10_countries_by_coverage = country_total_articles_per_capita.tail(10).sort_values()\n",
    "bottom_10_countries_by_coverage_df = bottom_10_countries_by_coverage.reset_index()\n",
    "bottom_10_countries_by_coverage_df.columns = ['Country', 'Total Articles per Capita']\n",
    "\n",
    "print(\"Bottom 10 Countries by Coverage: \\n\", bottom_10_countries_by_coverage_df, \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table genererated provides the \"Top 10 countries by high quality\": The 10 countries with the highest high quality articles per capita (in descending order) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Countries by High Quality:\n",
      "                  Country  High Quality Articles per Capita\n",
      "0              Lithuania                      4.137931e-06\n",
      "1                Albania                      2.592593e-06\n",
      "2                 Kosovo                      2.352941e-06\n",
      "3                Belarus                      1.413043e-06\n",
      "4                Croatia                      1.315789e-06\n",
      "5  Palestinian Territory                      1.090909e-06\n",
      "6               Slovenia                      9.523810e-07\n",
      "7            South Sudan                      8.108108e-07\n",
      "8            Switzerland                      7.954545e-07\n",
      "9                 Serbia                      7.575758e-07 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "country_high_quality_articles_per_capita = (country_high_quality_articles / (country_population * 1e6)).sort_values(ascending=False)\n",
    "top_10_countries_by_high_quality = country_high_quality_articles_per_capita.head(10)\n",
    "top_10_countries_by_high_quality_df = top_10_countries_by_high_quality.reset_index()\n",
    "top_10_countries_by_high_quality_df.columns = ['Country', 'High Quality Articles per Capita']\n",
    "print(\"Top 10 Countries by High Quality:\\n\", top_10_countries_by_high_quality_df, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table genererated provides the \"Bottom 10 countries by high quality\": The 10 countries with the lowest high quality articles per capita (in ascending order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottom 10 Countries by High Quality (High Quality Articles Per Capita):\n",
      "         Country  High Quality Articles per Capita\n",
      "0        Taiwan                               NaN\n",
      "1      Tanzania                               NaN\n",
      "2   Timor Leste                               NaN\n",
      "3          Togo                               NaN\n",
      "4        Turkey                               NaN\n",
      "5  Turkmenistan                               NaN\n",
      "6    Uzbekistan                               NaN\n",
      "7         Yemen                               NaN\n",
      "8        Zambia                               NaN\n",
      "9      Zimbabwe                               NaN \n",
      "\n"
     ]
    }
   ],
   "source": [
    "bottom_10_countries_by_high_quality = country_high_quality_articles_per_capita.tail(10).sort_values()\n",
    "bottom_10_countries_by_high_quality_df = bottom_10_countries_by_high_quality.reset_index()\n",
    "bottom_10_countries_by_high_quality_df.columns = ['Country', 'High Quality Articles per Capita']\n",
    "print(\"Bottom 10 Countries by High Quality (High Quality Articles Per Capita):\\n\", bottom_10_countries_by_high_quality_df, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now do similar analysis at Region level.\n",
    "The table generated shows \"Geographic regions by total coverage\": A rank ordered list of geographic regions (in descending order) by total articles per capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geographic Regions by Total Coverage (Total Articles Per Capita):\n",
      "              Region  High Quality Articles per Capita\n",
      "0           OCEANIA                      4.666067e-05\n",
      "1   SOUTHERN EUROPE                      3.931383e-05\n",
      "2   NORTHERN EUROPE                      3.419809e-05\n",
      "3         CARIBBEAN                      3.399763e-05\n",
      "4      WESTERN ASIA                      2.838224e-05\n",
      "5   CENTRAL AMERICA                      2.583798e-05\n",
      "6    EASTERN AFRICA                      1.931662e-05\n",
      "7    EASTERN EUROPE                      1.911848e-05\n",
      "8    WESTERN EUROPE                      1.342514e-05\n",
      "9     SOUTH AMERICA                      9.380543e-06\n",
      "10     CENTRAL ASIA                      8.937757e-06\n",
      "11  NORTHERN AFRICA                      7.732353e-06\n",
      "12    MIDDLE AFRICA                      5.570297e-06\n",
      "13   WESTERN AFRICA                      4.481440e-06\n",
      "14   SOUTHEAST ASIA                      3.596195e-06\n",
      "15  SOUTHERN AFRICA                      2.540853e-06\n",
      "16       SOUTH ASIA                      1.766950e-06\n",
      "17        EAST ASIA                      6.358125e-07 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate article counts per region and population\n",
    "region_population = final_df.groupby('Region')['Population'].mean()\n",
    "region_total_articles = final_df.groupby('Region').size()\n",
    "region_high_quality_articles = final_df[final_df['quality_prediction'].isin(high_quality_classes)].groupby('Region').size()\n",
    "region_total_articles_per_capita = (region_total_articles / (region_population * 1e6)).sort_values(ascending=False)\n",
    "region_total_articles_per_capita_df = region_total_articles_per_capita.reset_index()\n",
    "region_total_articles_per_capita_df.columns = ['Region', 'High Quality Articles per Capita']\n",
    "print(\"Geographic Regions by Total Coverage (Total Articles Per Capita):\\n\", region_total_articles_per_capita_df, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table generated shows \"Geographic regions by high quality coverage\": Rank ordered list of geographic regions (in descending order) by high quality articles per capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geographic Regions by High Quality Coverage (High Quality Articles Per Capita):\n",
      "              Region  High Quality Articles per Capita\n",
      "0   NORTHERN EUROPE                      2.795036e-06\n",
      "1   SOUTHERN EUROPE                      2.368852e-06\n",
      "2         CARIBBEAN                      1.384519e-06\n",
      "3    EASTERN EUROPE                      1.346021e-06\n",
      "4   CENTRAL AMERICA                      1.272807e-06\n",
      "5      WESTERN ASIA                      1.228078e-06\n",
      "6           OCEANIA                      6.480648e-07\n",
      "7    EASTERN AFRICA                      6.457589e-07\n",
      "8   NORTHERN AFRICA                      5.663825e-07\n",
      "9    WESTERN EUROPE                      5.538859e-07\n",
      "10    SOUTH AMERICA                      3.132343e-07\n",
      "11     CENTRAL ASIA                      2.692096e-07\n",
      "12   SOUTHEAST ASIA                      2.208965e-07\n",
      "13    MIDDLE AFRICA                      1.937495e-07\n",
      "14  SOUTHERN AFRICA                      1.652587e-07\n",
      "15   WESTERN AFRICA                      1.135648e-07\n",
      "16       SOUTH ASIA                      5.393307e-08\n",
      "17        EAST ASIA                      1.230605e-08 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "region_high_quality_articles_per_capita = (region_high_quality_articles / (region_population * 1e6)).sort_values(ascending=False)\n",
    "region_high_quality_articles_per_capita_df = region_high_quality_articles_per_capita.reset_index()\n",
    "region_high_quality_articles_per_capita_df.columns = ['Region', 'High Quality Articles per Capita']\n",
    "print(\"Geographic Regions by High Quality Coverage (High Quality Articles Per Capita):\\n\", region_high_quality_articles_per_capita_df, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
